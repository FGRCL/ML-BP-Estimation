{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.vitaldb.fetchingstrategy.Sdk import Sdk\n",
    "from src.vitaldb.casegenerator import VitalFileOptions\n",
    "from src.vitaldb.casesplit import split_generator\n",
    "import src.preprocessing.transforms as transforms\n",
    "import src.preprocessing.filters as filters\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The generator should have between 1 and 6388 case ids but got 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m batching \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m      8\u001B[0m options \u001B[38;5;241m=\u001B[39m VitalFileOptions(\n\u001B[1;32m      9\u001B[0m     [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSNUADC/ART\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39mfrequency\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m train_generator, val_generator, test_generator \u001B[38;5;241m=\u001B[39m \u001B[43msplit_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSdk\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.15\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m dataset_train \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_generator(\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m: train_generator,\n\u001B[1;32m     17\u001B[0m     output_signature\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m     18\u001B[0m         tf\u001B[38;5;241m.\u001B[39mTensorSpec(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m2\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[1;32m     19\u001B[0m     )\n\u001B[1;32m     20\u001B[0m )\n\u001B[1;32m     22\u001B[0m dataset_val \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_generator(\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m: val_generator,\n\u001B[1;32m     24\u001B[0m     output_signature\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m     25\u001B[0m         tf\u001B[38;5;241m.\u001B[39mTensorSpec(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m2\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[1;32m     26\u001B[0m     )\n\u001B[1;32m     27\u001B[0m )\n",
      "File \u001B[0;32m~/git/BPBaseline/src/vitaldb/casesplit.py:8\u001B[0m, in \u001B[0;36msplit_generator\u001B[0;34m(options, fetching_strategy, case_range, split_percentages)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit_generator\u001B[39m(options: VitalFileOptions, fetching_strategy: VitalDBFetchingStrategy, case_range: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m], split_percentages: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[VitalDBGenerator]:\n\u001B[1;32m      7\u001B[0m     splits \u001B[38;5;241m=\u001B[39m split_case_ids(case_range, split_percentages)\n\u001B[0;32m----> 8\u001B[0m     generators \u001B[38;5;241m=\u001B[39m [VitalDBGenerator(options, fetching_strategy, split) \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m splits]\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(generators)\n",
      "File \u001B[0;32m~/git/BPBaseline/src/vitaldb/casesplit.py:8\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit_generator\u001B[39m(options: VitalFileOptions, fetching_strategy: VitalDBFetchingStrategy, case_range: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m], split_percentages: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[VitalDBGenerator]:\n\u001B[1;32m      7\u001B[0m     splits \u001B[38;5;241m=\u001B[39m split_case_ids(case_range, split_percentages)\n\u001B[0;32m----> 8\u001B[0m     generators \u001B[38;5;241m=\u001B[39m [\u001B[43mVitalDBGenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetching_strategy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m splits]\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(generators)\n",
      "File \u001B[0;32m~/git/BPBaseline/src/vitaldb/casegenerator.py:19\u001B[0m, in \u001B[0;36mVitalDBGenerator.__init__\u001B[0;34m(self, options, fetching_strategy, case_ids)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, options: VitalFileOptions, fetching_strategy: VitalDBFetchingStrategy, case_ids: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m]):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(case_ids) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(case_ids) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m6388\u001B[39m: \u001B[38;5;66;03m#TODO save those constants somewhere\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe generator should have between 1 and 6388 case ids but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(case_ids)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcase_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(case_ids)\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions \u001B[38;5;241m=\u001B[39m options\n",
      "\u001B[0;31mException\u001B[0m: The generator should have between 1 and 6388 case ids but got 0"
     ]
    }
   ],
   "source": [
    "frequency = 500\n",
    "samples = range(1, 8)\n",
    "train_split = 0.7\n",
    "validate_split = 0.15\n",
    "validate_test = 0.15\n",
    "batching = False\n",
    "\n",
    "options = VitalFileOptions(\n",
    "    ['SNUADC/ART'],\n",
    "    1/frequency\n",
    ")\n",
    "\n",
    "train_generator, val_generator, test_generator = split_generator(options, Sdk(), samples, [0.7, 0.15, 0.15])\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 2), dtype=tf.float64)\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_generator(\n",
    "    lambda: val_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 2), dtype=tf.float64)\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def abp_low_pass_graph_adapter(x, frequency):\n",
    "    return tf.numpy_function(transforms.abp_low_pass, [x, frequency], tf.float64)\n",
    "\n",
    "def extract_clean_windows_graph_adapter(x, frequency: int, window_size: int, step_size: int):\n",
    "    return tf.numpy_function(transforms.extract_clean_windows, [x, frequency, window_size, step_size], tf.float64)\n",
    "\n",
    "def preprocess_dataset(dataset: tf.data.Dataset):\n",
    "    dataset = dataset.filter(filters.has_data)\n",
    "    dataset = dataset.map(transforms.extract_abp_track)\n",
    "    dataset = dataset.map(transforms.remove_nan)\n",
    "    dataset = dataset.map(lambda x: abp_low_pass_graph_adapter(x, frequency))\n",
    "    dataset = dataset.map(lambda x: extract_clean_windows_graph_adapter(x, frequency, 8, 2))\n",
    "    dataset= dataset.flat_map(transforms.to_tensor)\n",
    "    dataset = dataset.filter(lambda x: filters.pressure_out_of_bounds(x, 30, 230))\n",
    "    dataset = dataset.map(transforms.extract_sbp_dbp_from_abp_window)\n",
    "    dataset = dataset.map(transforms.normalize_array)\n",
    "\n",
    "    if batching:\n",
    "        dataset = dataset.map(lambda d, l: (tf.reshape(d, shape=(4000, 1)), l))\n",
    "        dataset = dataset.batch(20).prefetch(2)\n",
    "    else:\n",
    "        dataset = dataset.map(lambda d, l: (tf.reshape(d, shape=(1, 4000)), l))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset_train = preprocess_dataset(dataset_train)\n",
    "dataset_val = preprocess_dataset(dataset_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(4000, 1))\n",
    "x = layers.Conv1D(64, 15, activation='relu', input_shape=(4000, 1))(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling1D(4)(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.LSTM(64)(x)\n",
    "outputs = layers.Dense(2)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss=keras.losses.MeanAbsoluteError())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "model.fit(dataset_train, epochs=1, callbacks=[tensorboard_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate(dataset_val,  callbacks=[tensorboard_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}