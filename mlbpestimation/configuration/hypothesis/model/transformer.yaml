_target_: mlbpestimation.models.transformer.Transformer
sequence_length: 10
embedding_size: 100
n_layers: 4
n_attention_heads: 8
ff_units: 512
coder_dropout: 0.1
ff_dropout: 0.1
attention_dropout: 0.1
output_size: 2